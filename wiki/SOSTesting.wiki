#summary SOS Testing Process and Tools
#sidebar SOSTableOfContents

<wiki:toc max_depth="6" />

= Overview and Test Plan Objectives =

The IOOS SOS software reference implementations based on 52North and ncSOS are now complete, and other implementations are in progress.  IOOS data providers that serve in situ data are making plans to implement one or more of these solutions at their locations.  Once these services are installed within an IOOS data provider's domain, they will need to be tested to ensure the SOS responses and data formats conform to specifications.

*_For now, these tests are intended to be initial service deployment tests, and are not intended to be automated or recurring service validation tests.  Based on the results of these tests we may choose to build automated and/or recurring service validation tests._*

*_Further, this Test Plan will not be applied to the initial 'system test surrogate' installations. Rather, a series of more basic operations will be applied to determine whether that 'system testing' has passed._*

The objective of this test plan is to describe the overall testing strategy and to define how each data provider's service endpoint will be tested. This test plan will describe the following:

  * Scope of the tests to be conducted
  * Roles and responsibilities 
  * Testing process, methods and required tools
  * Success criteria for each test performed
  * Methodology for reporting defects or anomalies
  * Methodology for configuration management and version control
  * Format and content of Test Reports

The final section of the plan is intended to be used as a repository for ideas for additional test tools, scripts, etc.  This section should be augmented during test execution, and will potentially be used as requirements for future tool development.


= Scope =

SOS implementations at each data provider will be tested to ensure the service:

  # Is registered in the IOOS Service Registry
  # Is compliant with selected OGC specifications
  # Has a service response which is syntactically valid, i.e. the service response contain all expected elements and attributes in the order that is prescribed by the OGC specification AND IOOS Milestone 1.0 templates
  # Has a service response which is semantically valid, i.e. the response metadata conforms to standards, conventions, vocabularies, definitions, etc.
  # Contains an appropriate quantity of a data provider's in situ data offerings (80% of applicable data types)


See Tests and Success Criteria for more detailed info on specific tests to be conducted.


= Prerequisites =

Before this Test Plan can be used effectively, the following items need to be in place:

  # Reference implementation developers need to go through the 1.0 template schemas and review/correct anything that would cause problems during validation.
  # Complete the asset_sos_map
  # Implement "versions" field in ncSOS and 52N for configuration management


= Roles and Responsibilities =

|| *Role* || *Responsible Org/Individual* ||
||Test Authority||Derrick Snowden||
||Test Manager||Alex Birger||
||Tester||Varies based on test, likely Alex Birger will do majority||
||Software-related Issues||52N-Shane, ncSOS-Kyle, python-Emilio||
||Installation or data-related Issues||RA or Fed DMAC POC||
||Test Reports||Alex Birger||


= Process =

The general process is as follows:
  # Data provider POC announces a service endpoint is ready for testing, supplies the following information:
    # Endpoint URL
    # Software release or version number
    # Point of contact for test-related issues
  # Tester(s) execute tests on service. Defects are identified and logged in defect reporting system (Issues portion of ioostech).
  # Defects are investigated and addressed by appropriate individual.
  # Software or system updates are performed as needed to correct issues identified during testing.
  # Test Manager determines which tests need to be re-run, testing is re-done.
  # Process continues until Test Manager determines:
    # Service passed tests
    # Service cannot be further tested
  # Test results are documented in Test Report.
  # Test Authority makes final determination of service Pass or Fail.


= SOS Service Test States = 

Throughout the test process, services will transition through several states, as defined below.  The Fielding Plan (dashboard) will be updated weekly to reflect the latest state of each service.

|| *State* || *Meaning* ||
||Pending||Service is not ready for testing||
||Ready for Test||Service is available for test, testing has not begun||
||In Progress||Testing is underway||
||Hold||Testing identified defects; awaiting resolution before testing can continue||
||Complete-Success||Testing is complete and success criteria was met||
||Complete-Fail||Testing is complete, service did not meet success criteria||


= Test Methods and Success Criteria =

The sections below provide a high level description of the tests to be conducted.  A individual test is considered successful if the test outcome matches the Expected Result.


==Test 1: Service Registered in IOOS Service Registry==

*Test Goal:* Verify that the service is registered in the IOOS Service Registry.

*Test Method:*  Find service in NGDC service registry using the existing web interface.

*Expected Result:* Service under test is in the service registry and can be harvested.


==Test 2: OGC Compliance==

*Test Goal:* Ensure that the service is compliant with selected OGC specifications.

*Test Method:*  

The following operations will be tested using the OGC CITE test scripts:
  * GetCapabilities
  * DescribeSensor
  * GetObservation

*Expected Result:* No catastrophic errors encountered.


==Test 3: IOOS SOS Milestone 1.0 Template Compliance==

*Test Goal:*  Validate that the service response is syntactically valid, i.e. the service response contain all expected elements and attributes in the order that is prescribed by the IOOS Milestone 1.0 templates.

*Test Method:*
For each SOS Milestone 1.0 Template -
  # Issue command and capture XML response
  # Using Oxygen, perform schema validation on the response files using the [https://code.google.com/p/ioostech/source/browse/#svn%2Ftrunk%2Ftemplates%2FMilestone1.0%2FSchemas%253Fstate%253Dclosed IOOS XML Schemas for SOS Milestone 1.0 responses] that Alex developed based on 1.0 templates

*Expected Result:* Output files validate against schemas.



==Test 4: Compliance With IOOS Conventions==

*Test Goal:* Validate that the service has a response which is semantically valid, i.e. the response metadata conforms to standards, conventions, vocabularies, definitions, etc codified on the ioostech [SOSGuidelines#Rules wiki].  *Note: some of these rules are already codified as part of the Milestone 1.0 templates.  So we need to define which rules would be tested here.*

*Test Method:*
For each "rule" on the [SOSGuidelines#Rules wiki] -
  * Observe the appropriate service behavior for compliance with the rules.  This will likely initially be done by "eyeballing" the response.

*Expected Result:* Service behaves according to rule.  


==Test 5: Quantity of Offerings==

*Test Goal:* Verify that the service contains an appropriate quantity of a data provider's in situ data offerings.  The goal is to have 80% of available offerings accessible from the service. 

*Test Method:* Execute GetCapabilities operation on the service to determine how many datasets are served.  Report results to the appropriate DMAC coordinator and determine percentage of offerings that are served through the SOS.

*Expected Result:* At least 80% of data provider's offerings are served through the SOS.


= Defect Reporting =

Any issues encountered during testing will be logged in the Issues section of the [https://code.google.com/p/ioostech/issues/list wiki].

The defect list will be reviewed weekly by the Test Manager and IOOS DMAC personnel at the weekly DMAC Project Management meeting.

_How do we "assign" a defect to an individual?_
_Should we use Labels as a way to categorize the issues?_


= Configuration Management =

The software implementations have version numbers that are provided when the service endpoint is reported to be ready for test.  Testers will track software versions when running tests and reporting defects or issues.  


= Test Report Format =

No standalone test reports are planned for this series of tests.  Existing reporting mechanisms will be used to report progress, including:
  * [https://code.google.com/p/ioostech/issues/list Defect reports logged and tracked as wiki Issues]
  * The [https://sites.google.com/a/noaa.gov/ioos/collaboration-tools/dmac/fy2013/sosbuildout SOS Buildout] status intranet page


= Future Test Tool Needs/Requirements =

Include here ideas for test tools, scripts, utilities, etc that would enhance or automate the testing process.

----
 