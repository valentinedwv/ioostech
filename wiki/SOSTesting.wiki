#summary SOS Testing Process and Tools
#sidebar SOSTableOfContents

<wiki:toc max_depth="6" />

= Overview and Test Plan Objectives =

The IOOS SOS software reference implementations based on 52North and ncSOS are now complete, and other implementations are in progress.  IOOS data providers that serve in situ data are making plans to implement one or more of these solutions at their locations.  Once these services are installed within an IOOS data provider's domain, they will need to be tested to ensure the SOS responses and data formats conform to specifications.  

The objective of this test plan is to describe the overall testing strategy and to define how each data provider's service endpoint will be tested. This test plan will describe the following:

  * Scope of the tests to be conducted
  * Roles and responsibilities 
  * Testing process and required tools
  * Success criteria for each test performed
  * Methodology for reporting defects or anomalies
  * Methodology for configuration management and version control
  * Format and content of Test Reports


= Scope =

SOS implementations at each data provider will be tested to ensure the service:

  # Is online and capable of responding to requests
  # Is compliant with applicable OGC specifications
  # Has a service response which is syntactically valid, i.e. the service response contain all expected elements and attributes in the order that is prescribed by the OGC specification AND IOOS Milestone 1.0 templates
  # Has a service response which is semantically valid, i.e. the response metadata conforms to standards, conventions, vocabularies, definitions, etc.
  # Contains an appropriate quantity of a data provider's in situ data offerings 
  # Exhibits the same behavior as other SOS implementations in the case where a data provider is operating more than one SOS implementation (i.e. 52N and ncSOS)

See Tests and Success Criteria for more detailed info on specific tests to be conducted.

= Roles and Responsibilities =

||Role||Responsible Org/Individual||
||Software-related Issues||52N-Shane, ncSOS-Kyle||

= Process and Test Tools =

  # 

= Tests and Success Criteria =

= Defect Reporting =

= Configuration Management =

= Test Report Format =

= Risks and Issues =

new material cut/paste from the internal google site page.  1st step toward consolidating and simplifying the material.

[https://github.com/axiomalaska/ioos-sos-validator SOS Validator Tool]  Shane can make it run all the tests suggested by Alex:
# OGC compliance
  # Test output files for template compliance
# "Rules" from [SOSGuidelines#Rules wiki] (We need to determine the mandatory/optional tests.)
# For automation, how do we know which services to test?  Shane prefers to get SOS endpoints from the Service Registry via CSW, but may not be possible.  May need to manually populate a list of services to be tested.  Shane would have to be in the loop to add/delete services.
# Need to document Tool Requirements in the following form: [Name of Test] [xpath of element] [feature type] [Request] [expected outcome] [severity of failure]
  # List of tests should be a google spreadsheet with the fields defined above.
  # Only IOOS-specific tests need to be in this spreadsheet (not OGC stuff)
  # Also need to define format/location of Test Report.  Probably just a list of tests and pass/fail.  If fail, what severity of failure.
  # Wiki will maintain the list of tests that are to be executed with the validator.

Material below to be reworked.
= Introduction =

Initial testing is designed more towards a survey of existing SOS GetCapabilities, DescribeSensor and GetObservation services that have been submitted to the CatalogEndpoints document/spreadsheet.  For initial purposes, just looking at one (hopefully representative) SOS from each RA or other agency.  Also for initial purposes only considering single in-situ representations and not profiles or trajectories.

[#discussion_thread_1 See discussion_thread_1]

An initial simple perl script was setup to read submitted SOS endpoints and detail the number of platforms and observation types and their locations based on the GetCapabilities document.  The script also parses based on whether full xml namespaces are used and reports the vocabulary prefix in effect and provides a kml file of the platform locations.

This script can be used along with Eric Bridger's similar browser application which provides similar information at http://www.google.com/url?sa=D&q=http://dev.neracoos.org/IOOSCatalog3/SOSTester/catalog_end_points.html&usg=AFQjCNGG65xpqLiu3iCdtNebN2NxlQX_BA

[#discussion_thread_2 See discussion thread 2]

In addition to the above summary SOS information, creating a xsd(xml schema document) from a provided xml was used (the 'trang' tool) to compare existing schema/element differences between the existing SOS GetCapabilities documents.

Testing and categorization/compliance would be broken into 3 parts
  * a survey and categorization step to identify,categorize and describe existing SOS services to a known or new reference
    * see CatalogEndpoints spreadsheet and [SOSVersions]
  * differences in schema  - xsd reference and differences
    * see 'discussion thread 2' below
  * differences in vocabulary - the listing/range of possible element/prefix terms and differences

This script can be used along with Eric Bridger's similar browser application which provides similar information at http://dev.neracoos.org/IOOSCatalog3/SOSTester/catalog_end_points.html  (starfish/starfish)

See also [SOSSurvey] and [SOSVersions]

= SOS Assessment =

Summary of the current level of services and possibilities for evaluation, in an effort to identify requirements or problems to solve. 

== Criteria for Comparison ==
 
  # Does the service support compound offerings such as vector data (e.g. `observedProperty=winds` actually returns three or four scalar quantities such as wind_gust, wind_speed, wind_direction etc)?
  # Collections (e.g. `offering=network-all` returns all buoys in the network) and multiple observed properties (`observedProperty=sea_water_temperature,sea_water_salinity` returns data from both variables with one call)?
  # Correct usage of vocabularies?
  # Are the terms referenced by vocabularies, actually part of the voacabularies?  e.g. `<observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/winds"/>`, winds is not actually part of the CF Standard Name vocabulary despite the URL.  
  # How are resource IDs implemented in the response types?  (This questions is trying to ge at the unique ID problem but isn't stated well.)
  # Which SOS responses are supported?  DescribeSensor? SOS Core Profile, Transactional Profile or Full Profile?
  # Are you eating your own dog food or is this just for “IOOS compliance”?  Do you or any customers you know of actually use your SOS service?  If not, do you know the chief complaint?
  # What is the length of data available from each service (full time series from a ten year record or just the last month or year or whatever)?
  # Are data available on the service representative of the entire RA inventory?  If not, why?  Are these decisions related to technology impediments that we can solve?

== Questions/Comments about the assessment ==
Let's make sure each question has a purpose.  What we're envisioning here is a survey where all members of the team will need to spend time answering a questionnaire.  This is potentially non-trivial so let's make it worthwhile. How will we collect and/or compile the information?  
  * A template wiki page with questions?  Then copy/paste onto a new wiki page for each respondent?  
  * A linked spreadsheet?

Place holder for the work Jeremy started: [https://groups.google.com/d/topic/ioostech_dev/rnngO1AK0Ug/discussion "Thanks for posting the Catalog service endpoints document, from that I went through and tried to choose one SOS from each of the 11 RA's and federal backbone (ndbc,coop,ace) to start some analysis on. Worked up an initial perl script to get the GetCapabilites document... "].


=Discussion Threads=

==discussion thread 1==

from initial thread http://groups.google.com/group/ioostech_dev/browse_thread/thread/ae79e03b500ad148

Thanks for posting the Catalog service endpoints document, from that I went through and tried to choose one SOS from each of the 11 RA's and federal backbone(ndbc,coop,ace) to start some analysis on. 

Worked up an initial perl script to get the GetCapabilites document and had to manually tweak some of the response xml namespace info at the document beginning to get around some odd things which the version of LibXML I'm using doesn't handle well. 

The perl script is at http://neptune.baruch.sc.edu/xenia/ioos/get_sos.pl 
and the resultant GetCapabilities documents and processed files are at 
http://neptune.baruch.sc.edu/xenia/ioos/out 

Each organization in the current array list below like 'secoora' has 
a 'latest.txt.secoora' response corresponding to the getCapabilites 
response a summary file 'out_secoora' which is listed in a way for further post-processing - including platform locations and obs types, summary platform and observation type counts and kml/kmz file like 'secoora.kml' 'secoora.kmz' showing the platform 
locations and obs types at each location.
 
I wanted to start also processing a sample getObservation from each RA and Fed SOS to start profiling common points and differences similarly - if there is a spreadsheet of getObservation links for each provider and associated documentation or we can build one, let me know. 

Let me know if this type of analysis is helpful towards some kind of 
validation testing or if it overlaps with something already in place - I was also looking into perl or unix command-line tools for validating 
response xml's against an xml schema - xmllint and LibXML provide a basic pass/fail option towards this, but not sure if they work well in practice or provide enough useful detail on errors. 

In developing the perl script, the following differences were noted in the 
code below: 
########################################## 
#differences between sos's 
{{{
my $sos_ns_prefix = 'sos:'; 
if ($org eq 'gcoos' || $org eq 'macoora' || $org eq 'coop') { 
  $sos_ns_prefix = ''; 
} 

##### 
my $vocab_delimiter = '#'; 
if ($org eq 'gcoos' || $org eq 'cencoos' || $org eq 'pacioos' || $org eq 
'coop' || $org eq 'ace' || $org eq 'ndbc') { 
  $vocab_delimiter = '/'; 
} 

if ($org eq 'neracoos' || $org eq 'aoos') { 
  $vocab_delimiter = ':'; 
} 

##### 
#uri vocab prefixes 
#pacioos,cencoos,ndbc,coop,ace 
#http://mmisw.org/ont/cf/parameter/ 
#gcoos 
#http://mmisw.org/ont/gcoos/parameter/ 
#nanoos,sccoos,macoora,glos 
# 
http://www.csc.noaa.gov/ioos/schema/IOOS-DIF/IOOS/0.6.1/dictionaries/... 
#neracoos 
#urn:ogc:def:phenomenon:mmisw.org:cf: 
#aoos 
#urn:ogc:def:phenomenon:OGC:1.0.30:SwellPeriod 
#secoora 
#http://carolinasrcoos.org/cf# 
}}}

==discussion thread 2==

from http://groups.google.com/group/ioostech_dev/browse_thread/thread/f07d530a07fbf708

Posted some samples(will copy post to upcoming 'testing' wiki page 
also) some examples of the generated xsd files(secoora_sos.xsd,ndbc_sos.xsd,...) for SOS GetCapabilities 
documents using the 'trang' tool at http://neptune.baruch.sc.edu/xenia/sos/ioos/out/xsd 

Using NDBC's GetCapabilities as a base reference - doing a difference 
with the other xsd were almost same except for a few minor differences 
in the the following organizations - secoora,neracoos,maracoos,aoos,pacioos - each of which has a `*_diff` 
file to highlight the differences(like secoora_diff) 

Plan to go through this xsd generation and diff comparison for the 
DescribeSensor and GetObservations documents as well. 

I used a basic perl regex substitution script 
http://neptune.baruch.sc.edu/xenia/sos/ioos/out/filter/convert.sh to 
apply full 'sos:' namespaces to documents which didn't have them - and 
had to mess around with the namespace declarations, setting 
xmlns:sos="http://blah.org" to get trang to create a correct xsd file. 