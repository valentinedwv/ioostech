#summary SOS Testing Process and Tools
#sidebar SOSTableOfContents

<wiki:toc max_depth="6" />

= Overview and Test Plan Objectives =

The IOOS SOS software reference implementations based on 52North and ncSOS are now complete, and other implementations are in progress.  IOOS data providers that serve in situ data are making plans to implement one or more of these solutions at their locations.  Once these services are installed within an IOOS data provider's domain, they will need to be tested to ensure the SOS responses and data formats conform to specifications.

*_For now, these tests are intended to be initial service deployment tests, and are not intended to be automated or recurring service validation tests.  Based on the results of these tests we may choose to build automated and/or recurring service validation tests._*

*_Further, this Test Plan will not be applied to the initial 'system test surrogate' installations. Rather, a series of more basic operations will be applied to determine whether that 'system testing' has passed._*

The objective of this test plan is to describe the overall testing strategy and to define how each data provider's service endpoint will be tested. This test plan will describe the following:

  * Scope of the tests to be conducted
  * Roles and responsibilities 
  * Testing process, methods and required tools
  * Success criteria for each test performed
  * Methodology for reporting defects or anomalies
  * Methodology for configuration management and version control
  * Format and content of Test Reports

The final section of the plan is intended to be used as a repository for ideas for additional test tools, scripts, etc.  This section should be augmented during test execution, and will potentially be used as requirements for future tool development.

= Scope =

SOS implementations at each data provider will be tested to ensure the service:

  # Is online and capable of responding to requests
  # Is registered in the IOOS Service Registry
  # Is compliant with selected OGC specifications
  # Has a service response which is syntactically valid, i.e. the service response contain all expected elements and attributes in the order that is prescribed by the OGC specification AND IOOS Milestone 1.0 templates
  # Has a service response which is semantically valid, i.e. the response metadata conforms to standards, conventions, vocabularies, definitions, etc.
  # Contains an appropriate quantity of a data provider's in situ data offerings (80% of applicable data types)
  # Exhibits the same behavior as other SOS implementations in the case where a data provider is operating more than one SOS implementation (i.e. 52N and ncSOS)

See Tests and Success Criteria for more detailed info on specific tests to be conducted.

= Roles and Responsibilities =

||*Role*||*Responsible Org/Individual*||
||Test Authority||Derrick Snowden||
||Test Manager||Alex Birger||
||Tester||Varies based on test, likely Alex Birger will do majority||
||Software-related Issues||52N-Shane, ncSOS-Kyle, python-Emilio||
||Installation or data-related Issues||RA or Fed DMAC POC||
||Test Reports||Alex Birger||


= Process =

The general process is as follows:
  # Data provider POC announces a service endpoint is ready for testing, supplies the following information:
    # Endpoint URL
    # Software release or version number
    # Point of contact for test-related issues
  # Tester(s) execute tests on service. Defects are identified and logged in defect reporting system (Issues portion of ioostech).
  # Defects are investigated and addressed by appropriate individual.
  # Software or system updates are performed as needed to correct issues identified during testing.
  # Test Manager determines which tests need to be re-run, testing is re-done.
  # Process continues until Test Manager determines:
    # Service passed tests
    # Service cannot be further tested
  # Test results are documented in Test Report.
  # Test Authority makes final determination of service Pass or Fail.

= SOS Service Test States = 

Throughout the test process, services will transition through several states, as defined below.  The Fielding Plan (dashboard) will be updated weekly to reflect the latest state of each service.

||*State*||*Meaning*||
||Pending||Service is not ready for testing||
||Ready||Service is available for test, testing has not begun||
||In Progress||Testing is underway||
||Hold||Testing identified defects; awaiting resolution before testing can continue||
||Complete-Success||Testing is complete and success criteria was met||
||Complete-Fail||Testing is complete, service did not meet success criteria||

= Test Methods and Success Criteria =

The sections below provide a high level description of the tests to be conducted.  A individual test is considered successful if the test outcome matches the Expected Result.

==Test 1: Basic Service Response==
    
Test Goal: Verify that the service is online and capable of responding to requests.

Test Method:

Issue the following requested from a web browser:
  *GetCapabilities
  *DescribeSensor
  *GetObservation

Expected Result: Response XML received for each operation, with no exception errors.


==Test 2: Service Registered in IOOS Service Registry==

Test Goal: Verify that the service is registered in the IOOS Service Registry.

Test Method:  Find service in NGDC service registry using the existing web interface.

Expected Result: Service under test is in the service registry and can be harvested.


==Test 3: OGC Compliance==

Test Goal: Ensure that the service is compliant with selected OGC specifications.

Test Method:  

The following operations will be tested using the OGC CITE test scripts:
  *GetCapabilities
  *DescribeSensor
  *GetObservation

Expected Result: No catastrophic errors encountered.
_*Need to come up with a list of errors/exceptions and deteriment which ones would result in failure of the test.*_


==Test 4: IOOS SOS Milestone 1.0 Template Compliance==

Test Goal:  Validate that the service response is syntactically valid, i.e. the service response contain all expected elements and attributes in the order that is prescribed by the IOOS Milestone 1.0 templates.

Test Method:
For each SOS Milestone 1.0 Template -
  # Issue command and capture XML response
  # Using Oxygen, perform schema validation on the response files using the [https://code.google.com/p/ioostech/source/browse/#svn%2Ftrunk%2Ftemplates%2FMilestone1.0%2FSchemas%253Fstate%253Dclosed IOOS XML Schemas for SOS Milestone 1.0 responses] that Alex developed based on 1.0 templates

Expected Result: Output files validate against schemas.

==Test 5: Compliance With IOOS Conventions==

Has a service response which is semantically valid, i.e. the response metadata conforms to standards, conventions, vocabularies, definitions, etc:  adheres to the "rules" on the wiki.   need to determine which of those rules are important to test. answer should be based on which ones are needed for auto asset inventory.
"Rules" from [SOSGuidelines#Rules wiki] (We need to determine the mandatory/optional tests.)

==Test 6: Quantity of Offerings==

Contains an appropriate quantity of a data provider's in situ data offerings (80% of applicable data types): Determine what/how many datasets are served - use GetCapabilities - and then query the DMAC coordinator if that matches their intent.

==Test 7: Consistency of Behavior Across Implementations==

Exhibits the same behavior as other SOS implementations in the case where a data provider is operating more than one SOS implementation (i.e. 52N and ncSOS)


= Defect Reporting =

Any issues encountered during testing will be logged in the Issues section of ioostech: https://code.google.com/p/ioostech/issues/list

The defect list will be reviewed weekly by the Test Manager and IOOS DMAC personnel at the weekly DMAC Project Management meeting.

_How do we "assign" a defect to an individual?_
_Should we use Labels as a way to categorize the issues?_

= Configuration Management =

The software implementations have version numbers that are provided when the service endpoint is reported to be ready for test.  Testers will track software versions when running tests and reporting defects or issues.  

_When provided with just a service endpoint, is it possible to verify the software version?_

= Test Report Format =

= Future Test Tool Needs/Requirements =

Include here ideas for test tools, scripts, utilities, etc that would enhance or automate the testing process.

----

new material cut/paste from the internal google site page.  1st step toward consolidating and simplifying the material.

[https://github.com/axiomalaska/ioos-sos-validator SOS Validator Tool]  Shane can make it run all the tests suggested by Alex:
# OGC compliance
  # Test output files for template compliance
# "Rules" from [SOSGuidelines#Rules wiki] (We need to determine the mandatory/optional tests.)
# For automation, how do we know which services to test?  Shane prefers to get SOS endpoints from the Service Registry via CSW, but may not be possible.  May need to manually populate a list of services to be tested.  Shane would have to be in the loop to add/delete services.
# Need to document Tool Requirements in the following form: [Name of Test] [xpath of element] [feature type] [Request] [expected outcome] [severity of failure]
  # List of tests should be a google spreadsheet with the fields defined above.
  # Only IOOS-specific tests need to be in this spreadsheet (not OGC stuff)
  # Also need to define format/location of Test Report.  Probably just a list of tests and pass/fail.  If fail, what severity of failure.
  # Wiki will maintain the list of tests that are to be executed with the validator.

Material below to be reworked.
= Introduction =

Initial testing is designed more towards a survey of existing SOS GetCapabilities, DescribeSensor and GetObservation services that have been submitted to the CatalogEndpoints document/spreadsheet.  For initial purposes, just looking at one (hopefully representative) SOS from each RA or other agency.  Also for initial purposes only considering single in-situ representations and not profiles or trajectories.

[#discussion_thread_1 See discussion_thread_1]

An initial simple perl script was setup to read submitted SOS endpoints and detail the number of platforms and observation types and their locations based on the GetCapabilities document.  The script also parses based on whether full xml namespaces are used and reports the vocabulary prefix in effect and provides a kml file of the platform locations.

This script can be used along with Eric Bridger's similar browser application which provides similar information at http://www.google.com/url?sa=D&q=http://dev.neracoos.org/IOOSCatalog3/SOSTester/catalog_end_points.html&usg=AFQjCNGG65xpqLiu3iCdtNebN2NxlQX_BA

[#discussion_thread_2 See discussion thread 2]

In addition to the above summary SOS information, creating a xsd(xml schema document) from a provided xml was used (the 'trang' tool) to compare existing schema/element differences between the existing SOS GetCapabilities documents.

Testing and categorization/compliance would be broken into 3 parts
  * a survey and categorization step to identify,categorize and describe existing SOS services to a known or new reference
    * see CatalogEndpoints spreadsheet and [SOSVersions]
  * differences in schema  - xsd reference and differences
    * see 'discussion thread 2' below
  * differences in vocabulary - the listing/range of possible element/prefix terms and differences

This script can be used along with Eric Bridger's similar browser application which provides similar information at http://dev.neracoos.org/IOOSCatalog3/SOSTester/catalog_end_points.html  (starfish/starfish)

See also [SOSSurvey] and [SOSVersions]

= SOS Assessment =

Summary of the current level of services and possibilities for evaluation, in an effort to identify requirements or problems to solve. 

== Criteria for Comparison ==
 
  # Does the service support compound offerings such as vector data (e.g. `observedProperty=winds` actually returns three or four scalar quantities such as wind_gust, wind_speed, wind_direction etc)?
  # Collections (e.g. `offering=network-all` returns all buoys in the network) and multiple observed properties (`observedProperty=sea_water_temperature,sea_water_salinity` returns data from both variables with one call)?
  # Correct usage of vocabularies?
  # Are the terms referenced by vocabularies, actually part of the voacabularies?  e.g. `<observedProperty xlink:href="http://mmisw.org/ont/cf/parameter/winds"/>`, winds is not actually part of the CF Standard Name vocabulary despite the URL.  
  # How are resource IDs implemented in the response types?  (This questions is trying to ge at the unique ID problem but isn't stated well.)
  # Which SOS responses are supported?  DescribeSensor? SOS Core Profile, Transactional Profile or Full Profile?
  # Are you eating your own dog food or is this just for “IOOS compliance”?  Do you or any customers you know of actually use your SOS service?  If not, do you know the chief complaint?
  # What is the length of data available from each service (full time series from a ten year record or just the last month or year or whatever)?
  # Are data available on the service representative of the entire RA inventory?  If not, why?  Are these decisions related to technology impediments that we can solve?

== Questions/Comments about the assessment ==
Let's make sure each question has a purpose.  What we're envisioning here is a survey where all members of the team will need to spend time answering a questionnaire.  This is potentially non-trivial so let's make it worthwhile. How will we collect and/or compile the information?  
  * A template wiki page with questions?  Then copy/paste onto a new wiki page for each respondent?  
  * A linked spreadsheet?

Place holder for the work Jeremy started: [https://groups.google.com/d/topic/ioostech_dev/rnngO1AK0Ug/discussion "Thanks for posting the Catalog service endpoints document, from that I went through and tried to choose one SOS from each of the 11 RA's and federal backbone (ndbc,coop,ace) to start some analysis on. Worked up an initial perl script to get the GetCapabilites document... "].


=Discussion Threads=

==discussion thread 1==

from initial thread http://groups.google.com/group/ioostech_dev/browse_thread/thread/ae79e03b500ad148

Thanks for posting the Catalog service endpoints document, from that I went through and tried to choose one SOS from each of the 11 RA's and federal backbone(ndbc,coop,ace) to start some analysis on. 

Worked up an initial perl script to get the GetCapabilites document and had to manually tweak some of the response xml namespace info at the document beginning to get around some odd things which the version of LibXML I'm using doesn't handle well. 

The perl script is at http://neptune.baruch.sc.edu/xenia/ioos/get_sos.pl 
and the resultant GetCapabilities documents and processed files are at 
http://neptune.baruch.sc.edu/xenia/ioos/out 

Each organization in the current array list below like 'secoora' has 
a 'latest.txt.secoora' response corresponding to the getCapabilites 
response a summary file 'out_secoora' which is listed in a way for further post-processing - including platform locations and obs types, summary platform and observation type counts and kml/kmz file like 'secoora.kml' 'secoora.kmz' showing the platform 
locations and obs types at each location.
 
I wanted to start also processing a sample getObservation from each RA and Fed SOS to start profiling common points and differences similarly - if there is a spreadsheet of getObservation links for each provider and associated documentation or we can build one, let me know. 

Let me know if this type of analysis is helpful towards some kind of 
validation testing or if it overlaps with something already in place - I was also looking into perl or unix command-line tools for validating 
response xml's against an xml schema - xmllint and LibXML provide a basic pass/fail option towards this, but not sure if they work well in practice or provide enough useful detail on errors. 

In developing the perl script, the following differences were noted in the 
code below: 
########################################## 
#differences between sos's 
{{{
my $sos_ns_prefix = 'sos:'; 
if ($org eq 'gcoos' || $org eq 'macoora' || $org eq 'coop') { 
  $sos_ns_prefix = ''; 
} 

##### 
my $vocab_delimiter = '#'; 
if ($org eq 'gcoos' || $org eq 'cencoos' || $org eq 'pacioos' || $org eq 
'coop' || $org eq 'ace' || $org eq 'ndbc') { 
  $vocab_delimiter = '/'; 
} 

if ($org eq 'neracoos' || $org eq 'aoos') { 
  $vocab_delimiter = ':'; 
} 

##### 
#uri vocab prefixes 
#pacioos,cencoos,ndbc,coop,ace 
#http://mmisw.org/ont/cf/parameter/ 
#gcoos 
#http://mmisw.org/ont/gcoos/parameter/ 
#nanoos,sccoos,macoora,glos 
# 
http://www.csc.noaa.gov/ioos/schema/IOOS-DIF/IOOS/0.6.1/dictionaries/... 
#neracoos 
#urn:ogc:def:phenomenon:mmisw.org:cf: 
#aoos 
#urn:ogc:def:phenomenon:OGC:1.0.30:SwellPeriod 
#secoora 
#http://carolinasrcoos.org/cf# 
}}}

==discussion thread 2==

from http://groups.google.com/group/ioostech_dev/browse_thread/thread/f07d530a07fbf708

Posted some samples(will copy post to upcoming 'testing' wiki page 
also) some examples of the generated xsd files(secoora_sos.xsd,ndbc_sos.xsd,...) for SOS GetCapabilities 
documents using the 'trang' tool at http://neptune.baruch.sc.edu/xenia/sos/ioos/out/xsd 

Using NDBC's GetCapabilities as a base reference - doing a difference 
with the other xsd were almost same except for a few minor differences 
in the the following organizations - secoora,neracoos,maracoos,aoos,pacioos - each of which has a `*_diff` 
file to highlight the differences(like secoora_diff) 

Plan to go through this xsd generation and diff comparison for the 
DescribeSensor and GetObservations documents as well. 

I used a basic perl regex substitution script 
http://neptune.baruch.sc.edu/xenia/sos/ioos/out/filter/convert.sh to 
apply full 'sos:' namespaces to documents which didn't have them - and 
had to mess around with the namespace declarations, setting 
xmlns:sos="http://blah.org" to get trang to create a correct xsd file. 