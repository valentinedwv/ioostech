#summary Discussion of approaches for OGC SOS Service Validation with IOOS enhancements
#labels Phase-Design
#sidebar SOSTableOfContents

<wiki:toc max_depth="6" />


The US IOOS is developing and deploying the DMAC standards in order to ensure interoperable, quality-controlled ocean data that comes from various sources such as academia, federal agencies, private sector industries, or local governments. The standards shall be applied to the IOOS partners within the framework of the partners’ offerings quality verification process that should finally result in formal certification. 
Both federal and non-federal partners’ offerings need to be verified for compliance with DMAC standards in order for the provided data to be included into the US IOOS system. Non-federal partners may use the DMAC compliance for civil liability protection or just to ensure interoperability, and the formal certification process may be an identified means of verifying compliance.

=Scope=

The DMAC service development is in progress, and is almost completed. The implementations are about to start. This regimen is intended to examine the service and content of the DMAC implementation. 
The service validation seems to be an all-sufficient task, and can be done irrespective of other IOOS activity; however, since IOOS has carried out a development and implementation of the IOOS Registry, which among other functions is supposed to periodically probe partners’ services and harvest offered data, it would be highly beneficial to coordinate all testing activities (and especially any development of test tools) with IOOS Registry developers.
Formal verification of interoperability of data offerings is the core part of the compliance certification. The verification applies to both federal (NDBC, CO-OPS) and non-federal (RAs) partners, and includes series of tests of the partners’ offerings, as follows:

_TODO: Add text describing the differences between tests that are specific to OGC requiremenst and hence may be carried out by the OGC CITE tools and tests that are specific to IOOS requirements (e.g. use of vocabularies or use of correct identifier scheme).  Create another page for contributions of IOOS specific test._

 * *Data access service conformance* – verifies compliance with the OGC specification for SOS, SensorML, SWECommon, WMS, WCS, and OPeNDAP specification for DAP access service (it is assumed that both 52North and ncSOS varieties should be tested with the same tool): 
   1. _SOS, WCS, WMS, and SensorML_ – test engine and scripts developed by the OGC Compliance and Interoperability Testing Initiative (CITE); currently goes through an extensive modification and enhancement process; its growing popularity results in a rapid errors and bugs identification and correction. The CITE scripts [http://cite.opengeospatial.org/te2/about/sos/1.0.0/web do not test all features of the services]; thus, more [TestToolAvailability test tool development] is needed to fully cover the IOOS services functionality.<br>The following is list of currently available tests and their status:
|| *Specification* || *Version* || *Revision* || *Status* ||
||Geography Markup Language (GML)||3.2.1||3.2.1-`r2` ||Beta||
||Sensor Model Language (SensorML)||1.0.1||`r2` ||Beta||
||Sensor Observation Service (SOS)||1.0.0||`r6` ||Final||
||Sensor Observation Service (SOS)||2.0||`r1` ||Beta||
||Web Coverage Service (WCS)||1.0.0||`r5` ||Final||
||Web Coverage Service (WCS)||1.1.1||`r1` ||Final||
||Web Coverage Service (WCS)||2.0.1||`r2` ||Beta||
||Web Coverage Service - Earth Observation Profile||1.0 (pending)||`r2` ||Beta||
||Web Map Server (WMS) - Client||1.3.0||`r2` ||Beta||
||Web Map Service (WMS)||1.1.1||`r3` ||Final||
||Web Map Service (WMS)||1.3.0||`r4` ||Final||

No OGC CITE test for SWECommon is currently available, and probably will not been developed any time soon.
  
   2. _OPeNDAP_ – no standard tests are available; verification may be done with simple tools like [http://www.opendap.org/faq2#testMyServers Web browser and telnet client]. 

 * *Metadata conformance* – verifies compliance of metadata included in or accompanying requested datasets (files) with the IOOS endorsed standards (e.g. ISO-19115/19119/19139, CF Conventions, SensorML, vocabularies, etc.):
   # observations and model results in netCDF 
      ** visual analysis vs. [http://www.nodc.noaa.gov/data/formats/netcdf/ NODC templates] for each featureType;
      ** threddsISO or ncISO service (rubric)
      ** [http://puma.nerc.ac.uk/cgi-bin/cf-checker.pl CF-Convention Compliance Checker];
      ** else?
   # observations in SWECommon (SOS Response) 
      ** visual comparison with [https://code.google.com/p/ioostech/source/browse/#svn%2Ftrunk%2Ftemplates%2FMilestone1.0%253Fstate%253Dclosed IOOS SOS templates Milestone 1.0];
      ** validation against [https://code.google.com/p/ioostech/source/browse/#svn%2Ftrunk%2Ftemplates%2FMilestone1.0%2FSchemas%253Fstate%253Dclosed formal XML schemas] using XML Editor like Oxygen or specially developed validator;
      ** [https://www.ngdc.noaa.gov/metadata/published/NOAA/IOOS/iso/index-secure.html rubric service at NGDC] or IOOS RCV
      ** else?
   # MMI-hosted ontology – proper ontology test tool (?); possibly just visual observation of the data samples and comparison with the vocabularies
 * *Data quality* – methods and test tools are not clear at the moment


=Relevant documents and specifications=

For the purposes of this testing, the controlling documents for the DMAC services are (the list is open):
 * OGC SOS 1.0.0 Specification
 * OGC SOS 2.0 Specification (for Milestone 2.0)
 * OGC Observation & Measurement Specification
 * OGC Web Services Common Specification, Version 1.1.0 `[OGC 06-121r3]`
 * OGC SensorML v1.0 Specification
 * OGC SWECommon Data Model v2.0 (mostly for Milestone 2.0)
 * OGC OWS, O&M, SOS, etc. XML schemas
 * Ontologies (vocabularies, definitions, etc.) ([http://mmisw.org/orr/#b-http://mmisw.org/orr/] and [https://marinemetadata.org/references/ioosparameter-https://marinemetadata.org/references/ioosparameter])
 * SOS response templates (Milestone 1.0):
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/OM-GetObservation.xml OM-GetObservation.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SML-DescribeSensor-Network.xml SML-DescribeSensor-Network.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SML-DescribeSensor-Station.xml SML-DescribeSensor-Station.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SOS-GetCapabilities.xml SOS-GetCapabilities.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SWE-MultiStation-TimeSeries.xml SWE-MultiStation-TimeSeries.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SWE-MultiStation-TimeSeries_QC.xml SWE-MultiStation-TimeSeries_QC.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SWE-SingleStation-SingleProperty-TimeSeries.xml SWE-SingleStation-SingleProperty-TimeSeries.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SWE-SingleStation-TimeSeriesProfile.xml SWE-SingleStation-TimeSeriesProfile.xml]
   ** [http://code.google.com/p/ioostech/source/browse/trunk/templates/Milestone1.0/SWE-SingleStation-TimeSeriesProfile_QC.xml SWE-SingleStation-TimeSeriesProfile_QC.xml]
 * [https://code.google.com/p/ioostech/source/browse/#svn%2Ftrunk%2Ftemplates%2FMilestone1.0%2FSchemas%253Fstate%253Dclosed IOOS XML Schemas for SOS Milestone 1.0 responses]
 * List of entry points, and schedule of IOOS SOS implementation at the National Data Buoy Center (NDBC), Center for Operational Oceanographic Products and Services (CO-OPS), and 11 regions.


=Test goals=

The US IOOS test entity will use test tools – either standard and publicly available, or specifically developed for IOOS – to send requests to the services. The NBDC, CO-OPS, and RAs will be consulted to ensure that requests are reasonable and appropriate within the context of the DMAC Milestone 1.0.

Service requests and responses will be documented and analyzed in order to demonstrate the consistency, or lack thereof, of the responses from the services.

The following is the list of questions that validation should provide answers to, and available options of doing so. The list is by no means explicit, and should be revised and altered as needed:
 # Service state, i.e. “Is the service online and sending out products?”
   ** OGC CITE standard test
 # Service reaction to correct/incorrect requests, i.e. “To what degree is the service compliant with the OGC specification?”
   ** OGC CITE standard test ([http://cite.opengeospatial.org/te2/about/sos/1.0.0/web/ some requests are not tested])
 # Syntactic validity of the service response, i.e. “Does the service response contain all expected elements and attributes in the order that is prescribed by the OGC specification *AND* IOOS Milestone 1.0 templates?”
   ** OGC CITE standard test ([http://cite.opengeospatial.org/te2/about/sos/1.0.0/web/ some operations are not tested])
    ** Visual comparison between IOOS SOS XML response document and respective Milestone 1.0 template, or
    ** Validation of IOOS SOS XML response document with the respective XML Schema document (Schematron document?) – either manually with XML editor (e.g. Oxygen) or automatically with some scripts
 # Semantic validity of the service response, i.e. “Does the response metadata conform to standards, conventions, vocabularies, definitions, etc.?”
   ** Visual evaluation of the response
   ** Rubric tool at [https://www.ngdc.noaa.gov/metadata/published/NOAA/IOOS/iso/index-secure.html NGDC] or IOOS RCV (?) 
   ** else?
 # Validity of data provided by the service, i.e. “Are the quantitative values retrieved from the service and their distribution as expected (believable), and comparable to values from other services (if applicable)?”
   ** Manual analysis of the “result” section of the GetObservation response
   ** Automatic analysis (no automatic tool is available at the moment) (?)
 # Data missing values provided by the service, i.e. “Does the data in response products contain missing values, and do the count and geographic distribution of these values match what is expected (original data product)?”
   ** Manual analysis of the “result” section of the GetObservation response, or
   ** Automatic analysis (no automatic tool is available at the moment) (?)
 # else?


=Immediately Possible Test Activities=

 # Run OGC SOS compliance [TestToolAvailability test scripts]
 # In addition, run 52North SOS [http://sensorweb.demo.52north.org/52nSOSv3.2.1 test tool] (assumed that this tool will work on either ncSOS or 52North implementations).
 # Perform validation of IOOS SOS responses using the [http://code.google.com/p/ioostech/source/browse/#svn%2Ftrunk%2Ftemplates%2FMilestone1.0%2FSchemas schemas] developed from the Milestone 1.0 templates.
 # If possible, determine how real data offerings match data advertisement - use GetCapabilities and GetObservation responses - and then query the DMAC coordinator if that matches their intent.
 # If the RA is running both 52N and ncSOS, verify that both services behave the same way